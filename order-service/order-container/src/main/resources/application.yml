server:
  port: 8181

logging:
  level:
    com.food.ordering.system: DEBUG

order-service:
  payment-request-topic-name: payment-request
  payment-response-topic-name: payment-response
  restaurant-approval-request-topic-name: restaurant-approval-request
  restaurant-approval-response-topic-name: restaurant-approval-response

spring:
  jpa:
    # best performance holding connections
    open-in-view: false
    show-sql: true
    database-platform: org.hibernate.dialect.PostgreSQL9Dialect
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQL9Dialect
  datasource:
    # binaryTransfer = faster to transfer between the database and jdbc driver
    # reWriteBatchedInserts = faster in batched insert statement
    # stringtype = unspecified => save order id no problem in postgres
    url: jdbc:postgresql://localhost:5432/postgres?currentSchema=order&binaryTransfer=true&reWriteBatchedInserts=true&stringtype=unspecified
    username: postgres
    password: postgres
    driver-class-name: org.postgresql.Driver
    platform: postgres
    schema: classpath:init-schema.sql
    initialization-mode: always

kafka-config:
  bootstrap-servers: localhost:19092, localhost:29092, localhost:39092
  schema-registry-url-key: schema.registry.url
  schema-registry-url: http://localhost:8081
  num-of-partitions: 3
  replication-factor: 3

kafka-producer-config:
  key-serializer-class: org.apache.kafka.common.serialization.StringSerializer
  value-serializer-class: io.confluent.kafka.serializers.KafkaAvroSerializer
  # we have gzip (more compress but slower), lz4 (less compress but faster), snappy and zstd (medium compress and moderate performance)
  compression-type: snappy
  # 0 => faster but possible data loss, no brokers confirm
  # 1 => guaranteed data, at least 1 broker confirm
  # all => slower but no data loss, all brokers confirm
  acks: all
  # 16kb
  batch-size: 16384
  # how much data the consumer can hold
  batch-size-boost-factor: 100
  # delay on producer before send the data, default is 0 => send messages when they're ready
  linger-ms: 5
  request-timeout-ms: 60000
  retry-count: 5

kafka-consumer-config:
  key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
  payment-consumer-group-id: payment-topic-consumer
  restaurant-approval-consumer-group-id: restaurant-approval-topic-consumer
  customer-group-id: customer-topic-consumer
  # if no exists offset for this consumer in kafka, start from beginning. You can use latest to only get new data sent to topic
  auto-offset-reset: earliest
  specific-avro-reader-key: specific.avro.reader
  specific-avro-reader: true
  batch-listener: true
  auto-startup: true
  # create multiple threads to consume faster, 3 because we have 3 partitions
  concurrency-level: 3
  session-timeout-ms: 10000
  # frequency that the consumer send heartbeat signal to kafka for show he's alive (yes, I'm still alive)
  heartbeat-interval-ms: 3000
  max-poll-interval-ms: 300000
  max-poll-records: 500
  # 1 megabyte
  max-partition-fetch-bytes-default: 1048576
  max-partition-fetch-bytes-boost-factor: 1
  # do not use very short values here because has a looping in code fetched in this time. And not set long value because you'll block a thread
  poll-timeout-ms: 150
